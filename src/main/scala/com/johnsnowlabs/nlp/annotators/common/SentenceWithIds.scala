//package com.johnsnowlabs.nlp.annotators.common
//
//import com.johnsnowlabs.nlp.{Annotation, AnnotatorType}
//
//import scala.collection.Map
//
///**
//  * Class for containing ID based tokens generated by models like SentencePiece
//  * @param tokens
//  * @param sentenceId
//  */
////case class WordpieceEmbeddingsSentence
////(
////  tokens: Array[TokenPieceEmbeddings],
////  sentenceId: Int
////)
////
//
//case class SentencePieceTokenizedSentence(tokens: Array[TokenPiece])
//case class SentencePiece(sentence: String, token: String, pieceId: Int, isWordStart: Boolean, begin: Int, end: Int)
//
//case class SentenceWithIds(sentence: String, ids: Seq[Int], containsOOV: Boolean)
//
//object SentenceWithIds {
//  // TODO extend with actual string tokens. Use Overloaded Apply or make new Class...
//  def apply(sentence: String, ids: Seq[Int]): SentenceWithIds = {
//    SentenceWithIds(
//      sentence = sentence,
//      ids = ids,
//      containsOOV = false
//      )
//  }
//}
//
//object  WordpieceEmbeddingsSentence extends Annotated[WordpieceEmbeddingsSentence] {
//  override def annotatorType: String = AnnotatorType.ID_TOKENS
//
//  override def unpack(annotations: Seq[Annotation]): Seq[WordpieceEmbeddingsSentence] = {
//    val tokens = annotations
//      .filter(_.annotatorType == annotatorType)
//      .groupBy(_.metadata("sentence").toInt)
//
//    tokens.map{case (idx: Int, sentenceTokens: Seq[Annotation]) =>
//      val tokensWithSentence = sentenceTokens.map { token =>
//        new TokenPieceEmbeddings(
//          wordpiece = token.result,
//          token = token.metadata("token"),
//          pieceId = token.metadata("pieceId").toInt,
//          isWordStart = token.metadata("isWordStart").toBoolean,
//          isOOV = token.metadata.getOrElse("isOOV", "false").toBoolean,
//          embeddings = token.embeddings,
//          begin = token.begin,
//          end = token.end
//        )
//      }.toArray
//
//      WordpieceEmbeddingsSentence(tokensWithSentence, idx)
//    }.toSeq.sortBy(_.sentenceId)
//  }
//
//  override def pack(sentences: Seq[WordpieceEmbeddingsSentence]): Seq[Annotation] = {
//    sentences.flatMap{sentence =>
//      var isFirstToken = true
//      sentence.tokens.map{ token =>
//        // Store embeddings for token
//        val embeddings = token.embeddings
//
//        isFirstToken = false
//        Annotation(annotatorType, token.begin, token.end, token.token,
//          Map("sentence" -> sentence.sentenceId.toString,
//            "token" -> token.token,
//            "pieceId" -> token.pieceId.toString,
//            "isWordStart" -> token.isWordStart.toString,
//            "isOOV" -> token.isOOV.toString
//          ),
//          embeddings
//        )
//      }
//    }
//  }
//}
