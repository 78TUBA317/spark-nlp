---
layout: model
title: Elmo
author: John Snow Labs
name: elmo
class: ElmoEmbeddings
language: en
repository: public/models
date: 31/01/2020
tags: [embeddings]
article_header:
   type: cover
use_language_switcher: "Python-Scala-Java"
---

{:.h2_title}
## Description 
Computes contextualized word representations using character,based word representations and bidirectional LSTMs.,This model outputs fixed embeddings at each LSTM layer and a learnable aggregation of the 3 layers.,* `word_emb`: the character,based word representations with shape [batch_size, max_length, 512].  == word_emb,* `lstm_outputs1`: the first LSTM hidden state with shape [batch_size, max_length, 1024]. === lstm_outputs1,* `lstm_outputs2`: the second LSTM hidden state with shape [batch_size, max_length, 1024]. === lstm_outputs2,* `elmo`: the weighted sum of the 3 layers, where the weights are trainable. This tensor has shape [batch_size, max_length, 1024]  == elmo,The complex architecture achieves state of the art results on several benchmarks. Note that this is a very computationally expensive module compared to word embedding modules that only perform embedding lookups. The use of an accelerator is recommended.,The details are described in the paper "[Deep contextualized word representations](https://arxiv.org/abs/1802.05365)".





## How to use 
<div class="tabs-box" markdown="1">

{% include programmingLanguageSelectScalaPython.html %}

```python

```

```scala

```
</div>



{:.model-param}
## Model Information
{:.table-model}





{:.h2_title}
## Data Source
The model is imported from [https://tfhub.dev/google/elmo/3](https://tfhub.dev/google/elmo/3)

